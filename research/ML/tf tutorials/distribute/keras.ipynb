{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# Distributed training with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6P32iYYV27b"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/distribute/keras\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/distribute/keras.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The `tf.distribute.Strategy` API provides an abstraction for distributing your training across multiple processing units. It allows you to carry out distributed training using existing models and training code with minimal changes.\n",
        "\n",
        "This tutorial demonstrates how to use the `tf.distribute.MirroredStrategy` to perform in-graph replication with _synchronous training on many GPUs on one machine_. The strategy essentially copies all of the model's variables to each processor. Then, it uses [all-reduce](http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/) to combine the gradients from all processors, and applies the combined value to all copies of the model.\n",
        "\n",
        "You will use the `tf.keras` APIs to build the model and `Model.fit` for training it. (To learn about distributed training with a custom training loop and the `MirroredStrategy`, check out [this tutorial](custom_training.ipynb).)\n",
        "\n",
        "`MirroredStrategy` trains your model on multiple GPUs on a single machine. For _synchronous training on many GPUs on multiple workers_, use the `tf.distribute.MultiWorkerMirroredStrategy` with the [Keras Model.fit](multi_worker_with_keras.ipynb) or [a custom training loop](multi_worker_with_ctl.ipynb). For other options, refer to the [Distributed training guide](../../guide/distributed_training.ipynb).\n",
        "\n",
        "To learn about various other strategies, there is the [Distributed training with TensorFlow](../../guide/distributed_training.ipynb) guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dney9v7BsJij"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r8S3ublR7Ay8"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SkocY8tgRd3H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.13.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXhefksNKk2I"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtnnUwvmB3X5"
      },
      "source": [
        "Load the MNIST dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets). This returns a dataset in the `tf.data` format.\n",
        "\n",
        "Setting the `with_info` argument to `True` includes the metadata for the entire dataset, which is being saved here to `info`. Among other things, this metadata object includes the number of train and test examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iXMJ3G9NB3X6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:11:38.002723: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
            "2024-02-26 20:11:38.002747: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
            "2024-02-26 20:11:38.002753: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
            "2024-02-26 20:11:38.002806: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-02-26 20:11:38.002824: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
        "\n",
        "mnist_train, mnist_test = datasets['train'], datasets['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrjVhv-eKuHD"
      },
      "source": [
        "## Define the distribution strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlH8vx6BB3X9"
      },
      "source": [
        "Create a `MirroredStrategy` object. This will handle distribution and provide a context manager (`MirroredStrategy.scope`) to build your model inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4j0tdf4YB3X9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ]
        }
      ],
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cY3KA_h2iVfN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of devices: 1\n"
          ]
        }
      ],
      "source": [
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNbPv0yAleW8"
      },
      "source": [
        "## Set up the input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psozqcuptXhK"
      },
      "source": [
        "When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. In general, use the largest batch size that fits the GPU memory and tune the learning rate accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p1xWxKcnhar9"
      },
      "outputs": [],
      "source": [
        "# You can also do info.splits.total_num_examples to get the total\n",
        "# number of examples in the dataset.\n",
        "\n",
        "num_train_examples = info.splits['train'].num_examples\n",
        "num_test_examples = info.splits['test'].num_examples\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wm5rsL2KoDF"
      },
      "source": [
        "Define a function that normalizes the image pixel values from the `[0, 255]` range to the  `[0, 1]` range ([feature scaling](https://en.wikipedia.org/wiki/Feature_scaling)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Eo9a46ZeJCkm"
      },
      "outputs": [],
      "source": [
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZCa5RLc5A91"
      },
      "source": [
        "Apply this `scale` function to the training and test data, and then use the `tf.data.Dataset` APIs to shuffle the training data (`Dataset.shuffle`), and batch it (`Dataset.batch`). Notice that you are also keeping an in-memory cache of the training data to improve performance (`Dataset.cache`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gRZu2maChwdT"
      },
      "outputs": [],
      "source": [
        "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xsComp8Kz5H"
      },
      "source": [
        "## Create the model and instantiate the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BnQYQTpB3YA"
      },
      "source": [
        "Within the context of `Strategy.scope`, create and compile the model using the Keras API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IexhL_vIB3YA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCDKFcNJzdcd"
      },
      "source": [
        "For this toy example with the MNIST dataset, you will be using the Adam optimizer's default learning rate of 0.001.\n",
        "\n",
        "For larger datasets, the key benefit of distributed training is to learn more in each training step, because each step processes more training data in parallel, which allows for a larger learning rate (within the limits of the model and dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i6OU5W9Vy2u"
      },
      "source": [
        "## Define the callbacks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOXO5nvvK3US"
      },
      "source": [
        "Define the following [Keras Callbacks](https://www.tensorflow.org/guide/keras/train_and_evaluate):\n",
        "\n",
        "- `tf.keras.callbacks.TensorBoard`: writes a log for TensorBoard, which allows you to visualize the graphs.\n",
        "- `tf.keras.callbacks.ModelCheckpoint`: saves the model at a certain frequency, such as after every epoch.\n",
        "- `tf.keras.callbacks.BackupAndRestore`: provides the fault tolerance functionality by backing up the model and current epoch number. Learn more in the _Fault tolerance_ section of the [Multi-worker training with Keras](multi_worker_with_keras.ipynb) tutorial.\n",
        "- `tf.keras.callbacks.LearningRateScheduler`: schedules the learning rate to change after, for example, every epoch/batch.\n",
        "\n",
        "For illustrative purposes, add a [custom callback](https://www.tensorflow.org/guide/keras/custom_callback) called `PrintLR` to display the *learning rate* in the notebook.\n",
        "\n",
        "**Note:** Use the `BackupAndRestore` callback instead of `ModelCheckpoint` as the main mechanism to restore the training state upon a restart from a job failure. Since `BackupAndRestore` only supports eager mode, in graph mode consider using `ModelCheckpoint`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A9bwLCcXzSgy"
      },
      "outputs": [],
      "source": [
        "# Define the checkpoint directory to store the checkpoints.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Define the name of the checkpoint files.\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wpU-BEdzJDbK"
      },
      "outputs": [],
      "source": [
        "# Define a function for decaying the learning rate.\n",
        "# You can define any decay function you need.\n",
        "def decay(epoch):\n",
        "  if epoch < 3:\n",
        "    return 1e-3\n",
        "  elif epoch >= 3 and epoch < 7:\n",
        "    return 1e-4\n",
        "  else:\n",
        "    return 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jKhiMgXtKq2w"
      },
      "outputs": [],
      "source": [
        "# Define a callback for printing the learning rate at the end of each epoch.\n",
        "class PrintLR(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    print('\\nLearning rate for epoch {} is {}'.format(        epoch + 1, model.optimizer.lr.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YVqAbR6YyNQh"
      },
      "outputs": [],
      "source": [
        "# Put all the callbacks together.\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                       save_weights_only=True),\n",
        "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
        "    PrintLR()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70HXgDQmK46q"
      },
      "source": [
        "## Train and evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EophnOAB3YD"
      },
      "source": [
        "Now, train the model in the usual way by calling Keras `Model.fit` on the model and passing in the dataset created at the beginning of the tutorial. This step is the same whether you are distributing the training or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7MVw_6CqB3YD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:11:38.382937: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:551] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2024-02-26 20:11:38.435354: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:11:38.439274: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:11:38.470868: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:11:38.474705: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "2024-02-26 20:11:38.716709: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1/938 [..............................] - ETA: 13:51 - loss: 2.3119 - accuracy: 0.0469WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_train_batch_end` time: 0.0086s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_train_batch_end` time: 0.0086s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.9333INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Learning rate for epoch 1 is 0.0010000000474974513\n",
            "938/938 [==============================] - 10s 10ms/step - loss: 0.2222 - accuracy: 0.9333 - lr: 0.0010\n",
            "Epoch 2/12\n",
            "  6/938 [..............................] - ETA: 9s - loss: 0.1301 - accuracy: 0.9635 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:11:48.871711: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:11:48.876119: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "935/938 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9748\n",
            "Learning rate for epoch 2 is 0.0010000000474974513\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0869 - accuracy: 0.9747 - lr: 0.0010\n",
            "Epoch 3/12\n",
            " 18/938 [..............................] - ETA: 8s - loss: 0.0887 - accuracy: 0.9740"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:11:57.181553: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:11:57.185157: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "935/938 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9785\n",
            "Learning rate for epoch 3 is 0.0010000000474974513\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0724 - accuracy: 0.9785 - lr: 0.0010\n",
            "Epoch 4/12\n",
            " 11/938 [..............................] - ETA: 11s - loss: 0.0579 - accuracy: 0.9830"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:12:05.530797: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:12:05.534466: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "933/938 [============================>.] - ETA: 0s - loss: 0.0454 - accuracy: 0.9869\n",
            "Learning rate for epoch 4 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0454 - accuracy: 0.9869 - lr: 1.0000e-04\n",
            "Epoch 5/12\n",
            " 18/938 [..............................] - ETA: 8s - loss: 0.0566 - accuracy: 0.9852"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:12:13.764829: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:12:13.768498: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "936/938 [============================>.] - ETA: 0s - loss: 0.0408 - accuracy: 0.9884\n",
            "Learning rate for epoch 5 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0408 - accuracy: 0.9884 - lr: 1.0000e-04\n",
            "Epoch 6/12\n",
            " 19/938 [..............................] - ETA: 8s - loss: 0.0419 - accuracy: 0.9860"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:12:22.047050: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:12:22.050636: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "935/938 [============================>.] - ETA: 0s - loss: 0.0394 - accuracy: 0.9889\n",
            "Learning rate for epoch 6 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0394 - accuracy: 0.9889 - lr: 1.0000e-04\n",
            "Epoch 7/12\n",
            " 18/938 [..............................] - ETA: 8s - loss: 0.0457 - accuracy: 0.9826"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:12:30.265821: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:12:30.270010: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "934/938 [============================>.] - ETA: 0s - loss: 0.0382 - accuracy: 0.9889\n",
            "Learning rate for epoch 7 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0383 - accuracy: 0.9889 - lr: 1.0000e-04\n",
            "Epoch 8/12\n",
            " 12/938 [..............................] - ETA: 9s - loss: 0.0401 - accuracy: 0.9896 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:12:38.496891: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:12:38.500500: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "933/938 [============================>.] - ETA: 0s - loss: 0.0345 - accuracy: 0.9903\n",
            "Learning rate for epoch 8 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0344 - accuracy: 0.9903 - lr: 1.0000e-05\n",
            "Epoch 9/12\n",
            " 18/938 [..............................] - ETA: 8s - loss: 0.0288 - accuracy: 0.9905"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:12:46.770510: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:12:46.774642: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "935/938 [============================>.] - ETA: 0s - loss: 0.0340 - accuracy: 0.9905\n",
            "Learning rate for epoch 9 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0340 - accuracy: 0.9905 - lr: 1.0000e-05\n",
            "Epoch 10/12\n",
            " 18/938 [..............................] - ETA: 8s - loss: 0.0302 - accuracy: 0.9913"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:12:55.018038: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:12:55.022345: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "936/938 [============================>.] - ETA: 0s - loss: 0.0338 - accuracy: 0.9906\n",
            "Learning rate for epoch 10 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0338 - accuracy: 0.9906 - lr: 1.0000e-05\n",
            "Epoch 11/12\n",
            " 18/938 [..............................] - ETA: 8s - loss: 0.0431 - accuracy: 0.9870"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:13:03.295852: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:13:03.299593: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "935/938 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9905\n",
            "Learning rate for epoch 11 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0337 - accuracy: 0.9905 - lr: 1.0000e-05\n",
            "Epoch 12/12\n",
            " 18/938 [..............................] - ETA: 8s - loss: 0.0521 - accuracy: 0.9861"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:13:11.516739: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:13:11.521347: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9906\n",
            "Learning rate for epoch 12 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0336 - accuracy: 0.9906 - lr: 1.0000e-05\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x298e9f490>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EPOCHS = 12\n",
        "\n",
        "model.fit(train_dataset, epochs=EPOCHS, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUcWAUUupIvG"
      },
      "source": [
        "Check for saved checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JQ4zeSTxKEhB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoint                  ckpt_12.index\n",
            "ckpt-1.data-00000-of-00001  ckpt_2.data-00000-of-00001\n",
            "ckpt-1.index                ckpt_2.index\n",
            "ckpt-2.data-00000-of-00001  ckpt_3.data-00000-of-00001\n",
            "ckpt-2.index                ckpt_3.index\n",
            "ckpt-3.data-00000-of-00001  ckpt_4.data-00000-of-00001\n",
            "ckpt-3.index                ckpt_4.index\n",
            "ckpt-4.data-00000-of-00001  ckpt_5.data-00000-of-00001\n",
            "ckpt-4.index                ckpt_5.index\n",
            "ckpt-5.data-00000-of-00001  ckpt_6.data-00000-of-00001\n",
            "ckpt-5.index                ckpt_6.index\n",
            "ckpt_1.data-00000-of-00001  ckpt_7.data-00000-of-00001\n",
            "ckpt_1.index                ckpt_7.index\n",
            "ckpt_10.data-00000-of-00001 ckpt_8.data-00000-of-00001\n",
            "ckpt_10.index               ckpt_8.index\n",
            "ckpt_11.data-00000-of-00001 ckpt_9.data-00000-of-00001\n",
            "ckpt_11.index               ckpt_9.index\n",
            "ckpt_12.data-00000-of-00001\n"
          ]
        }
      ],
      "source": [
        "# Check the checkpoint directory.\n",
        "!ls {checkpoint_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qor53h7FpMke"
      },
      "source": [
        "To check how well the model performs, load the latest checkpoint and call `Model.evaluate` on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JtEwxiTgpQoP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-26 20:13:19.961469: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:551] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2024-02-26 20:13:20.016853: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:13:20.021017: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:13:20.063893: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:13:20.067596: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
            "2024-02-26 20:13:20.147310: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 7ms/step - loss: 0.0569 - accuracy: 0.9820\n",
            "Eval loss: 0.056857880204916, Eval accuracy: 0.9819999933242798\n"
          ]
        }
      ],
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
        "\n",
        "print('Eval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIeF2RWfYu4N"
      },
      "source": [
        "To visualize the output, launch TensorBoard and view the logs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vtyAZO0DoKu_"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-5b5b2dcec739ca6d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-5b5b2dcec739ca6d\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir=logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0a82d26d6bd"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/tensorboard_distributed_training_with_keras.png\"/> -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LnyscOkvKKBR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 0\n",
            "0 \u001b[34mtrain\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls -sh ./logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBLlogrDvMgg"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa87y_A0vRma"
      },
      "source": [
        "Save the model to a `.keras` zip archive using `Model.save`. After your model is saved, you can load it with or without the `Strategy.scope`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "h8Q4MKOLwG7K"
      },
      "outputs": [],
      "source": [
        "path = 'my_model.keras'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4HvcDmVsvQoa"
      },
      "outputs": [],
      "source": [
        "model.save(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKJT4w5JwVPI"
      },
      "source": [
        "Now, load the model without `Strategy.scope`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "T_gT0RbRvQ3o"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Adam' object has no attribute 'build'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unreplicated_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m unreplicated_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      4\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m      6\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m eval_loss, eval_acc \u001b[38;5;241m=\u001b[39m unreplicated_model\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset)\n",
            "File \u001b[0;32m~/anaconda3/envs/sameerkhanAI/lib/python3.8/site-packages/keras/src/saving/saving_api.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    239\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    240\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/sameerkhanAI/lib/python3.8/site-packages/keras/src/saving/saving_lib.py:275\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    272\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m~/anaconda3/envs/sameerkhanAI/lib/python3.8/site-packages/keras/src/saving/saving_lib.py:240\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 240\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
            "File \u001b[0;32m~/anaconda3/envs/sameerkhanAI/lib/python3.8/site-packages/keras/src/saving/serialization_lib.py:710\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m     compile_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compile_config:\n\u001b[0;32m--> 710\u001b[0m         \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompile_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[1;32m    713\u001b[0m     record_object_after_deserialization(\n\u001b[1;32m    714\u001b[0m         instance, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    715\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/envs/sameerkhanAI/lib/python3.8/site-packages/keras/src/engine/training.py:3582\u001b[0m, in \u001b[0;36mModel.compile_from_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m   3580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m   3581\u001b[0m     \u001b[38;5;66;03m# Create optimizer variables.\u001b[39;00m\n\u001b[0;32m-> 3582\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
            "File \u001b[0;32m~/anaconda3/envs/sameerkhanAI/lib/python3.8/site-packages/keras/src/optimizers/legacy/optimizer_v2.py:997\u001b[0m, in \u001b[0;36mOptimizerV2.__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyper:\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hyper(name)\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m~/anaconda3/envs/sameerkhanAI/lib/python3.8/site-packages/keras/src/optimizers/legacy/optimizer_v2.py:987\u001b[0m, in \u001b[0;36mOptimizerV2.__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Overridden to support hyperparameter access.\"\"\"\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# Needed to avoid infinite recursion with __setattr__.\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hyper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Adam' object has no attribute 'build'"
          ]
        }
      ],
      "source": [
        "unreplicated_model = tf.keras.models.load_model(path)\n",
        "\n",
        "unreplicated_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n",
        "\n",
        "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBLzcRF0wbDe"
      },
      "source": [
        "Load the model with `Strategy.scope`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBVo3WGGwd9a"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  replicated_model = tf.keras.models.load_model(path)\n",
        "  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                           optimizer=tf.keras.optimizers.Adam(),\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\n",
        "  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUZwaz4AKjtD"
      },
      "source": [
        "### Additional resources\n",
        "\n",
        "More examples that use different distribution strategies with the Keras `Model.fit` API:\n",
        "\n",
        "1. The [Solve GLUE tasks using BERT on TPU](https://www.tensorflow.org/text/tutorials/bert_glue) tutorial uses `tf.distribute.MirroredStrategy` for training on GPUs and `tf.distribute.TPUStrategy` on TPUs.\n",
        "1. The [Save and load a model using a distribution strategy](save_and_load.ipynb) tutorial demonstates how to use the SavedModel APIs with `tf.distribute.Strategy`.\n",
        "1. The [official TensorFlow models](https://github.com/tensorflow/models/tree/master/official) can be configured to run multiple distribution strategies.\n",
        "\n",
        "To learn more about TensorFlow distribution strategies:\n",
        "\n",
        "1. The [Custom training with tf.distribute.Strategy](custom_training.ipynb) tutorial shows how to use the `tf.distribute.MirroredStrategy` for single-worker training with a custom training loop.\n",
        "1. The [Multi-worker training with Keras](multi_worker_with_keras.ipynb) tutorial shows how to use the `MultiWorkerMirroredStrategy` with `Model.fit`.\n",
        "1. The [Custom training loop with Keras and MultiWorkerMirroredStrategy](multi_worker_with_ctl.ipynb) tutorial shows how to use the `MultiWorkerMirroredStrategy` with Keras and a custom training loop.\n",
        "1. The [Distributed training in TensorFlow](https://www.tensorflow.org/guide/distributed_training) guide provides an overview of the available distribution strategies.\n",
        "1. The [Better performance with tf.function](../../guide/function.ipynb) guide provides information about other strategies and tools, such as the [TensorFlow Profiler](../../guide/profiler.md) you can use to optimize the performance of your TensorFlow models.\n",
        "\n",
        "Note: `tf.distribute.Strategy` is actively under development and TensorFlow will be adding more examples and tutorials in the near future. Please give it a try. Your feedback is welcome—feel free to submit it via [issues on GitHub](https://github.com/tensorflow/tensorflow/issues/new)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "keras.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
